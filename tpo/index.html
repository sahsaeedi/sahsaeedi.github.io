<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="Monocular Depth Estimation, Language Guidance">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>TPO</title>
  
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<style>
.container {
  position: relative;
}

.text-block {
  position: relative;
  top: 0px;
  right: 0px;
  margin-left: 5px;
  width: 97.5%;
  text-align: center;
  border-radius:10px 10px 0px 0px;
  border: 1px solid #787878;
  background-color: #787878;
  color: white;
  padding-left: 0px;
  padding-right: 0px;
  padding-top: 3px;
  padding-bottom: 3px;
}

.row {
            display: flex;
            justify-content: center;
            gap: 10px; /* Adjust the gap between images */
        }
</style>
</head>
<body>

<nav class="navbar" style="margin-bottom:-40px" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2404.14723">
            Insights Into Alignments
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div style="margin-bottom:-80px" class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sahsaeedi.github.io/">Amir Saeidi</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=huOeftkAAAAJ&hl=en">Shivanshu Verma</a>,</span>
              <span class="author-block">
                <a href="">Aswin RRV</a>,</span>
			<span class="author-block">
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a></span>

          </div>


          <!-- <br> <center><h2 class="title is-3">CVPR 2024</h2></center><br> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sahsaeedi/triple-preference-optimization"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-3">Abstract</h2>
    <center>
      <p style="text-align: justify;">Large Language Models (LLMs) perform well across diverse tasks, but aligning them with human demonstrations is challenging. 
        Recently, Reinforcement Learning (RL)-free methods like Direct Preference Optimization (DPO) have emerged, offering improved stability and scalability while retaining competitive performance relative to RL-based methods.
        However, while RL-free methods deliver satisfactory performance, they require significant data to develop a robust Supervised Fine-Tuned (SFT) model and an additional step to fine-tune this model on a preference dataset, which constrains their utility and scalability.
        In this paper, we introduce <b>Triple Preference Optimization (TPO)</b>, a new preference learning method designed to align an LLM with three preferences without requiring a separate SFT step and using considerably less data.
        Through a combination of practical experiments and theoretical analysis, we show the efficacy of TPO as a single-step alignment strategy. 
        Specifically, we fine-tuned the Phi-2 (2.7B) and Mistral (7B) models using TPO directly on the UltraFeedback dataset, achieving superior results compared to models aligned through other methods such as SFT, DPO, KTO, IPO, CPO, and ORPO.
        Moreover, the performance of TPO without the SFT component led to notable improvements in the MT-Bench score, with increases of <b>+1.27</b> and <b>+0.63</b> over SFT and DPO, respectively. 
        Additionally, TPO showed higher average accuracy, surpassing DPO and SFT by <b>4.2%</b> and <b>4.97%</b> on the Open LLM Leaderboard benchmarks.</p>
        <div class="hero-body" style='margin-top:-25px;margin-bottom:-25px'>
		  <center>
		  <img src="./static/images/TPO-demonstration2.png"
					 class="interpolation-image" width=100%/></center>
          <p class="content has-text-justified" style='width:100%'>
            <b>(a)</b> During the SFT step, a pre-trained model is fine-tuned to align with human expectations. <b>(b)</b> To further enhance the performance of the SFT model, we train it with human preferences using reinforcement learning. <b>(c)</b> Alternatively, we can directly align an SFT model with human preferences using RL-free methods such as DPO. <b>(d)</b> In TPO, we merge preference optimization with gold standard response learning, enabling direct fine-tuning of a pre-trained model based on three preferences.
		  </p>		
        </div>
    </center>
      </div>
    </div>
	
  
</div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top:-70px">
    <h2 class="title">Experiments</h2>
    <p>
      We assessed the performance of <b>TPO</b> alongside SFT, DPO, KTO, IPO, CPO, and ORPO across ten different benchmarks in three distinct scenarios: <b>1)</b> Aligning an SFT model fine-tuned on 10K data, <b>2)</b> Aligning an SFT model fine-tuned on 200K data, and <b>3)</b> Directly fine-tuning a pre-trained model.
    </p>
    <h3 class="">1. Aligning an SFT Model</h3>

    <div><center>
		  <img src="./static/images/Leaderboard-SFT.png" class="" width="100%"/> </center> 

      <p class="content has-text-justified" style='width:100%'><br>
        Comparing TPO's performance with other alignment methods reveals that the <b>Mistral+TPO</b> model exhibits comparable performance across different benchmarks and, on average, outperforms other methods. In particular, <b>Mistral+TPO</b> performed remarkably on the TruthfulQA benchmark. It's worth noting that the <b>Mistral+TPO</b> model is directly trained with TPO, which contributes to its superior performance. Additionally, for all benchmarks, accuracy is the metric used to gauge performance.
      </p>	<br>
    </div>
      <div><center>
      <img src="./static/images/BB-SFT.png" class="" width="100%" /></center>
        <br>
      <p class="content has-text-justified" style='width:100%'>
        In our comparison of TPO with other alignment methods across more benchmarks, <b>Mistral+SFT+TPO</b> and <b>Mistral+TPO</b> emerge as the top performer, surpassing other methods in MT-Bench and BB-causal, BB-sports, OpenBookQA. For BB-causal, BB-sports, BB-formal, and OpenBookQA, performance is evaluated based on accuracy, while MT-Bench uses a scoring system generated by GPT-4.
    </p>
    </div>
    <br>
    <div><center>
      <img src="./static/images/Phi2-SFT.png" class="" width="80%" /></center>
        <br>
      <p class="content has-text-justified" style='width:100%'>
        The comparison of Phi-2's performance when aligned with various methods on MT-Bench shows that <b>Phi-2+TPO</b> surpasses other alignment techniques.
    </p>
    <br>
    </div>

    <div><center>
      <img src="./static/images/WithSFT.png" class="" width="50%" /></center>
        <br>
      <p class="content has-text-justified" style='width:100%'>
        Comparison of the performance of various alignment methods on different SFT models using the MT-Bench. Notably, the score for Mistral+SFT trained on 10K data is <b>4.2</b>, while the score for Mistral+SFT trained on 200K data is <b>5.94</b> and for <b>Mistral+TPO</b> on 10K is <b>6.66</b>.
    </p>
    </div>
	
    <h3 class="">2. Aligning without SFT Model</h3>

    <div><center>
		  <img src="./static/images/Without-SFT.png" class="" width="50%"/> </center> 

      <p class="content has-text-justified" style='width:100%'><br>
        Comparison of the performance of various alignment methods on MT-Bench.
      </p>	<br>
    </div>


  </div>
</section>




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top:-70px" >
    <h2 class="title">BibTeX</h2>
    
    <pre><code>@article{saeidi2024insights,
        title={Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks},
        author={Saeidi, Amir and Verma, Shivanshu and Baral, Chitta},
        journal={arXiv preprint arXiv:2404.14723},
        year={2024}
      }
    </code></pre>
    
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content" style="margin-top:-70px">
    <h2 class="title">Acknowledgement</h2>
    We thank the anonymous reviewers for constructive suggestions and the Research Computing (RC) at Arizona State University (ASU) for providing computing resources for experiments. We acknowledge support by a 2023 Spring Amazon Research Award (ARA).
    Template of this website is borrowed from <a href="https://nerfies.github.io/">nerfies</a> website.
</code></pre>
  </div>
</section>

</body>
</html>
